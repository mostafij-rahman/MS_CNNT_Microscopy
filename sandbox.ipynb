{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Check the Python environment used by Jupyter Notebook\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install the tqdm module\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install --upgrade jupyter\n",
    "!{sys.executable} -m pip install --upgrade ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Enable Jupyter widgets extension\n",
    "!{sys.executable} -m jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "!{sys.executable} -m jupyter nbextension install --py widgetsnbextension --sys-prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Extract the tar.gz file with progress bar and save files in the same parent directory\n",
    "import tarfile\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Path to your tar.gz file\n",
    "tar_path = '/isilon/ai-data/PublicData/Nonprocessed/Denoising_Planaria.tar.gz'\n",
    "\n",
    "# Get the parent directory of the tar.gz file\n",
    "parent_dir = os.path.dirname(tar_path)\n",
    "\n",
    "# Open the tar.gz file\n",
    "with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "    # Get the total number of files in the tar.gz for the progress bar\n",
    "    total_files = len(tar.getmembers())\n",
    "    print(total_files)\n",
    "    # Extract the tar.gz file with a progress bar\n",
    "    #with tqdm(total=total_files, desc=\"Extracting files\") as pbar:\n",
    "    for member in tar.getmembers():\n",
    "        tar.extract(member, path=parent_dir)\n",
    "        #pbar.update(1)\n",
    "\n",
    "print(\"Extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: List all TIFF files in the extracted directory\n",
    "import os\n",
    "\n",
    "# Directory containing the extracted TIFF files\n",
    "image_dir = '/isilon/ai-data/PublicData/Nonprocessed/Denoising_Planaria/train_data/'\n",
    "\n",
    "# List all TIFF files in the directory\n",
    "tiff_files = os.listdir(image_dir) #[f for f in os.listdir(image_dir) if f.endswith('.npz') or f.endswith('.tif')]\n",
    "\n",
    "print(\"TIFF files found:\", tiff_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and display the first TIFF image and its shape\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if there are any TIFF files\n",
    "if tiff_files:\n",
    "    # Load the first TIFF image\n",
    "    image_path = os.path.join(image_dir, tiff_files[0])\n",
    "    image = tiff.imread(image_path)\n",
    "    \n",
    "    # Display the shape of the image\n",
    "    print(\"Shape of the image:\", image.shape)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Microscopy Image')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No TIFF files found in the extracted directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def split_h5_file(input_file, output_dir, output_file1_name, output_file2_name):\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Construct the full output file paths\n",
    "    output_file1_path = os.path.join(output_dir, output_file1_name)\n",
    "    output_file2_path = os.path.join(output_dir, output_file2_name)\n",
    "    \n",
    "    test_data = {}\n",
    "    train_data = {}\n",
    "\n",
    "    # Open the input HDF5 file in read mode\n",
    "    with h5py.File(input_file, 'r') as infile:\n",
    "        keys = list(infile.keys())\n",
    "\n",
    "        # Determine the split index\n",
    "        split_index = 25\n",
    "\n",
    "        # Process top-level groups and datasets\n",
    "        for i, key in enumerate(keys):\n",
    "            item = infile[key]\n",
    "            if isinstance(item, h5py.Group):\n",
    "                if i < split_index:\n",
    "                    test_data[key] = {}\n",
    "                else:\n",
    "                    train_data[key] = {}\n",
    "                for subkey in item.keys():\n",
    "                    subitem = item[subkey]\n",
    "                    if isinstance(subitem, h5py.Dataset):\n",
    "                        data = subitem[:]\n",
    "                        if i < split_index:\n",
    "                            test_data[key][subkey] = data\n",
    "                            print(f\"Added {key}/{subkey} to test_data\")\n",
    "                        else:\n",
    "                            train_data[key][subkey] = data\n",
    "                            print(f\"Added {key}/{subkey} to train_data\")\n",
    "                    else:\n",
    "                        print(f\"Skipped {key}/{subkey} as it is not a dataset\")\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                data = item[:]\n",
    "                if i < split_index:\n",
    "                    test_data[key] = data\n",
    "                    print(f\"Added {key} to test_data\")\n",
    "                else:\n",
    "                    train_data[key] = data\n",
    "                    print(f\"Added {key} to train_data\")\n",
    "            else:\n",
    "                print(f\"Skipped {key} as it is not a group or dataset\")\n",
    "\n",
    "    def save_data(outfile, data_dict):\n",
    "        for key, data in data_dict.items():\n",
    "            if isinstance(data, dict):\n",
    "                group = outfile.create_group(key)\n",
    "                save_data(group, data)\n",
    "            else:\n",
    "                if data.dtype.kind == 'U':  # Check if data type is Unicode string\n",
    "                    data = np.array(data, dtype='S')  # Convert to byte string\n",
    "                outfile.create_dataset(key, data=data)\n",
    "                #print(f\"Saved {key} to {outfile.filename}\")\n",
    "\n",
    "    # Save the split data into the new HDF5 files\n",
    "    with h5py.File(output_file1_path, 'w') as outfile1:\n",
    "        save_data(outfile1, test_data)\n",
    "            \n",
    "    with h5py.File(output_file2_path, 'w') as outfile2:\n",
    "        save_data(outfile2, train_data)\n",
    "\n",
    "# Usage example\n",
    "root_dir = '/isilon/lab-xue/publications/CNNT_paper/data/micro_datasets_tvt_split/'\n",
    "input_file = os.path.join(root_dir, 'Ryo_tile_new_train.h5')\n",
    "output_file1_name = 'Ryo_tile_new_ft_train.h5'\n",
    "output_file2_name = 'Ryo_tile_new_backbone_train.h5'\n",
    "\n",
    "split_h5_file(input_file, root_dir, output_file1_name, output_file2_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/isilon/lab-xue/publications/CNNT_paper/data/micro_datasets_tvt_split/Ryo_tile_new_ft_train.h5'\n",
    "# Open the input HDF5 file in read mode\n",
    "with h5py.File(test_file, 'r') as test_data_new:\n",
    "    print(len(test_data_new))\n",
    "    print(test_data_new)\n",
    "    print(test_data_new.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n",
      "tensor(0.9219)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "preds = torch.rand([3, 1, 256, 256])\n",
    "target = preds * 0.75\n",
    "metric = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "values = [ ]\n",
    "for _ in range(10):\n",
    "    ssim = metric(preds, target)\n",
    "    print(ssim)\n",
    "    values.append(ssim)\n",
    "fig_, ax_ = metric.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "root_dir = '/isilon/lab-xue/publications/CNNT_paper/data/micro_datasets_tvt_split/'\n",
    "input_file = os.path.join(root_dir, 'Base_All_new_train.h5')\n",
    "output_file = os.path.join('/home/rahmanm9/micro_datasets_tvt_split_float32/', 'Base_All_new_train.h5')\n",
    "\n",
    "with h5py.File(input_file, 'r') as infile, h5py.File(output_file, 'w') as outfile:\n",
    "    keys = list(infile.keys())\n",
    "    \n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for key in keys:\n",
    "        noisy_data = np.asarray(infile[key + \"/noisy_im\"], dtype=np.float32)\n",
    "        clean_data = np.asarray(infile[key + \"/clean_im\"], dtype=np.float32)\n",
    "        \n",
    "        # Create groups and datasets in the output file\n",
    "        grp = outfile.create_group(key)\n",
    "        grp.create_dataset(\"noisy_im\", data=noisy_data, dtype='float32')\n",
    "        grp.create_dataset(\"clean_im\", data=clean_data, dtype='float32')\n",
    "    \n",
    "    # End measuring time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate and print elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM3D: 0.9600000381469727\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from torch import Tensor\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Union, Literal\n",
    "\n",
    "\n",
    "def _gaussian_kernel_3d(channel: int, kernel_size: Sequence[int], sigma: Sequence[float], dtype: torch.dtype, device: torch.device) -> Tensor:\n",
    "    \"\"\"Creates a 3D Gaussian kernel.\"\"\"\n",
    "    coords = [torch.arange(size, dtype=dtype, device=device) for size in kernel_size]\n",
    "    coords = [coord - (size - 1) / 2 for coord, size in zip(coords, kernel_size)]\n",
    "    kernel = torch.exp(-0.5 * sum((coord ** 2 / s ** 2 for coord, s in zip(coords, sigma))))\n",
    "    kernel = kernel / kernel.sum()\n",
    "    kernel = kernel.expand(channel, 1, *kernel_size)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def _reflection_pad_3d(x: Tensor, pad_d: int, pad_w: int, pad_h: int) -> Tensor:\n",
    "    \"\"\"Pads a 5D tensor with reflection padding.\"\"\"\n",
    "    return F.pad(x, (pad_w, pad_w, pad_h, pad_h, pad_d, pad_d), mode='reflect')\n",
    "\n",
    "\n",
    "def _ssim_check_inputs(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Update and returns variables required to compute Structural Similarity Index Measure.\"\"\"\n",
    "    if preds.dtype != target.dtype:\n",
    "        target = target.to(preds.dtype)\n",
    "    if preds.shape != target.shape:\n",
    "        raise ValueError(f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\")\n",
    "    if len(preds.shape) != 5:\n",
    "        raise ValueError(f\"Expected `preds` and `target` to have BxCxDxHxW shape, but got {preds.shape}.\")\n",
    "    return preds, target\n",
    "\n",
    "\n",
    "class StructuralSimilarityIndexMeasure3D(Metric):\n",
    "    \"\"\"Compute Structural Similarity Index Measure (SSIM) for 3D images.\"\"\"\n",
    "\n",
    "    higher_is_better: bool = True\n",
    "    is_differentiable: bool = True\n",
    "    full_state_update: bool = False\n",
    "    plot_lower_bound: float = 0.0\n",
    "    plot_upper_bound: float = 1.0\n",
    "\n",
    "    preds: List[Tensor]\n",
    "    target: List[Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gaussian_kernel: bool = True,\n",
    "        sigma: Union[float, Sequence[float]] = 1.5,\n",
    "        kernel_size: Union[int, Sequence[int]] = 11,\n",
    "        reduction: Literal[\"elementwise_mean\", \"sum\", \"none\", None] = \"elementwise_mean\",\n",
    "        data_range: Optional[Union[float, Tuple[float, float]]] = None,\n",
    "        k1: float = 0.01,\n",
    "        k2: float = 0.03,\n",
    "        return_full_image: bool = False,\n",
    "        return_contrast_sensitivity: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        valid_reduction = (\"elementwise_mean\", \"sum\", \"none\", None)\n",
    "        if reduction not in valid_reduction:\n",
    "            raise ValueError(f\"Argument `reduction` must be one of {valid_reduction}, but got {reduction}\")\n",
    "\n",
    "        if reduction in (\"elementwise_mean\", \"sum\"):\n",
    "            self.add_state(\"similarity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        else:\n",
    "            self.add_state(\"similarity\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "        if return_contrast_sensitivity or return_full_image:\n",
    "            self.add_state(\"image_return\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "        self.gaussian_kernel = gaussian_kernel\n",
    "        self.sigma = sigma\n",
    "        self.kernel_size = kernel_size\n",
    "        self.reduction = reduction\n",
    "        self.data_range = data_range\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.return_full_image = return_full_image\n",
    "        self.return_contrast_sensitivity = return_contrast_sensitivity\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor) -> None:\n",
    "        \"\"\"Update state with predictions and targets.\"\"\"\n",
    "        preds, target = _ssim_check_inputs(preds, target)\n",
    "        similarity_pack = self._ssim_update(preds, target)\n",
    "\n",
    "        if isinstance(similarity_pack, tuple):\n",
    "            similarity, image = similarity_pack\n",
    "        else:\n",
    "            similarity = similarity_pack\n",
    "\n",
    "        if self.return_contrast_sensitivity or self.return_full_image:\n",
    "            self.image_return.append(image)\n",
    "\n",
    "        if self.reduction in (\"elementwise_mean\", \"sum\"):\n",
    "            self.similarity += similarity.sum()\n",
    "            self.total += preds.shape[0]\n",
    "        else:\n",
    "            self.similarity.append(similarity)\n",
    "\n",
    "    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"Compute SSIM over state.\"\"\"\n",
    "        if self.reduction == \"elementwise_mean\":\n",
    "            similarity = self.similarity / self.total\n",
    "        elif self.reduction == \"sum\":\n",
    "            similarity = self.similarity\n",
    "        else:\n",
    "            similarity = torch.cat(self.similarity, dim=0)\n",
    "\n",
    "        if self.return_contrast_sensitivity or self.return_full_image:\n",
    "            image_return = torch.cat(self.image_return, dim=0)\n",
    "            return similarity, image_return\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def plot(\n",
    "        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[Any] = None\n",
    "    ) -> Any:\n",
    "        \"\"\"Plot a single or multiple values from the metric.\"\"\"\n",
    "        return self._plot(val, ax)\n",
    "\n",
    "    def _ssim_update(\n",
    "        self,\n",
    "        preds: Tensor,\n",
    "        target: Tensor\n",
    "    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"Compute Structural Similarity Index Measure.\"\"\"\n",
    "        if not isinstance(self.kernel_size, Sequence):\n",
    "            self.kernel_size = 3 * [self.kernel_size]\n",
    "        if not isinstance(self.sigma, Sequence):\n",
    "            self.sigma = 3 * [self.sigma]\n",
    "\n",
    "        if len(self.kernel_size) != 3:\n",
    "            raise ValueError(f\"`kernel_size` should have 3 dimensions, but got {len(self.kernel_size)}\")\n",
    "        if len(self.sigma) != 3:\n",
    "            raise ValueError(f\"`sigma` should have 3 dimensions, but got {len(self.sigma)}\")\n",
    "\n",
    "        if self.return_full_image and self.return_contrast_sensitivity:\n",
    "            raise ValueError(\"Arguments `return_full_image` and `return_contrast_sensitivity` are mutually exclusive.\")\n",
    "\n",
    "        if any(x % 2 == 0 or x <= 0 for x in self.kernel_size):\n",
    "            raise ValueError(f\"Expected `kernel_size` to have odd positive numbers. Got {self.kernel_size}\")\n",
    "\n",
    "        if any(y <= 0 for y in self.sigma):\n",
    "            raise ValueError(f\"Expected `sigma` to have positive numbers. Got {self.sigma}\")\n",
    "\n",
    "        if self.data_range is None:\n",
    "            self.data_range = max(preds.max() - preds.min(), target.max() - target.min())\n",
    "        elif isinstance(self.data_range, tuple):\n",
    "            preds = torch.clamp(preds, min=self.data_range[0], max=self.data_range[1])\n",
    "            target = torch.clamp(target, min=self.data_range[0], max=self.data_range[1])\n",
    "            self.data_range = self.data_range[1] - self.data_range[0]\n",
    "\n",
    "        #print(f\"Data Range: {self.data_range}\")\n",
    "\n",
    "        c1 = (self.k1 * self.data_range) ** 2\n",
    "        c2 = (self.k2 * self.data_range) ** 2\n",
    "        device = preds.device\n",
    "\n",
    "        #print(f\"C1: {c1}, C2: {c2}\")\n",
    "\n",
    "        channel = preds.size(1)\n",
    "        dtype = preds.dtype\n",
    "        gauss_kernel_size = [int(3.5 * s + 0.5) * 2 + 1 for s in self.sigma]\n",
    "\n",
    "        pad_h = (gauss_kernel_size[0] - 1) // 2\n",
    "        pad_w = (gauss_kernel_size[1] - 1) // 2\n",
    "        pad_d = (gauss_kernel_size[2] - 1) // 2\n",
    "\n",
    "        preds = _reflection_pad_3d(preds, pad_d, pad_w, pad_h)\n",
    "        target = _reflection_pad_3d(target, pad_d, pad_w, pad_h)\n",
    "\n",
    "        if self.gaussian_kernel:\n",
    "            kernel = _gaussian_kernel_3d(channel, gauss_kernel_size, self.sigma, dtype, device)\n",
    "        else:\n",
    "            kernel = torch.ones((channel, 1, *self.kernel_size), dtype=dtype, device=device) / torch.prod(\n",
    "                torch.tensor(self.kernel_size, dtype=dtype, device=device)\n",
    "            )\n",
    "\n",
    "        input_list = torch.cat((preds, target, preds * preds, target * target, preds * target))\n",
    "\n",
    "        outputs = F.conv3d(input_list, kernel, groups=channel)\n",
    "\n",
    "        output_list = outputs.split(preds.shape[0])\n",
    "\n",
    "        mu_pred = output_list[0]\n",
    "        mu_target = output_list[1]\n",
    "        mu_pred_sq = mu_pred.pow(2)\n",
    "        mu_target_sq = mu_target.pow(2)\n",
    "        mu_pred_target = mu_pred * mu_target\n",
    "\n",
    "        sigma_pred_sq = torch.clamp(output_list[2] - mu_pred_sq, min=0.0)\n",
    "        sigma_target_sq = torch.clamp(output_list[3] - mu_target_sq, min=0.0)\n",
    "        sigma_pred_target = torch.clamp(output_list[4] - mu_pred_target, min=0.0)\n",
    "\n",
    "        #print(f\"mu_pred: {mu_pred.mean().item()}, mu_target: {mu_target.mean().item()}, mu_pred_target: {mu_pred_target.mean().item()}\")\n",
    "        #print(f\"sigma_pred_sq: {sigma_pred_sq.mean().item()}, sigma_target_sq: {sigma_target_sq.mean().item()}, sigma_pred_target: {sigma_pred_target.mean().item()}\")\n",
    "\n",
    "        upper = 2 * sigma_pred_target.to(dtype) + c2\n",
    "        lower = (sigma_pred_sq + sigma_target_sq).to(dtype) + c2\n",
    "\n",
    "        #print(f\"Upper: {upper.mean().item()}, Lower: {lower.mean().item()}\")\n",
    "\n",
    "        ssim_idx_full_image = ((2 * mu_pred_target + c1) * upper) / ((mu_pred_sq + mu_target_sq + c1) * lower)\n",
    "\n",
    "        ssim_idx = ssim_idx_full_image[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]\n",
    "\n",
    "        if self.return_contrast_sensitivity:\n",
    "            contrast_sensitivity = upper / lower\n",
    "            contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]\n",
    "            return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), contrast_sensitivity.reshape(\n",
    "                contrast_sensitivity.shape[0], -1\n",
    "            ).mean(-1)\n",
    "\n",
    "        if self.return_full_image:\n",
    "            return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), ssim_idx_full_image\n",
    "\n",
    "        return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1)\n",
    "\n",
    "\n",
    "# Example Usage for 3D SSIM\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ssim3d = StructuralSimilarityIndexMeasure3D(data_range=1.0).to(device)\n",
    "\n",
    "    # Example 3D tensors (batch_size, channels, depth, height, width)\n",
    "    preds = torch.rand([3, 1, 32, 256, 256], device=device)\n",
    "    target = preds * 0.75\n",
    "\n",
    "    # Update and compute SSIM\n",
    "    ssim3d.update(preds, target)\n",
    "    ssim_value = ssim3d.compute()\n",
    "\n",
    "    print(f\"SSIM3D: {ssim_value.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM2D: 0.9218916893005371\n",
      "SSIM3D: 0.9600000381469727\n",
      "tensor(0.1253, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Metric\n",
    "from torch import Tensor\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Union, Literal\n",
    "\n",
    "\n",
    "def _gaussian_kernel_2d(channel: int, kernel_size: int, sigma: float, dtype: torch.dtype, device: torch.device) -> Tensor:\n",
    "    \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n",
    "    coords = torch.arange(kernel_size, dtype=dtype, device=device) - (kernel_size - 1) / 2\n",
    "    grid = torch.meshgrid(coords, coords, indexing=\"ij\")\n",
    "    kernel = torch.exp(-0.5 * (grid[0] ** 2 + grid[1] ** 2) / sigma ** 2)\n",
    "    kernel = kernel / kernel.sum()\n",
    "    kernel = kernel.expand(channel, 1, kernel_size, kernel_size)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def _gaussian_kernel_3d(channel: int, kernel_size: Sequence[int], sigma: Sequence[float], dtype: torch.dtype, device: torch.device) -> Tensor:\n",
    "    \"\"\"Creates a 3D Gaussian kernel.\"\"\"\n",
    "    coords = [torch.arange(size, dtype=dtype, device=device) for size in kernel_size]\n",
    "    coords = [coord - (size - 1) / 2 for coord, size in zip(coords, kernel_size)]\n",
    "    kernel = torch.exp(-0.5 * sum((coord ** 2 / s ** 2 for coord, s in zip(coords, sigma))))\n",
    "    kernel = kernel / kernel.sum()\n",
    "    kernel = kernel.expand(channel, 1, *kernel_size)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def _reflection_pad_2d(x: Tensor, pad_h: int, pad_w: int) -> Tensor:\n",
    "    \"\"\"Pads a 4D tensor with reflection padding.\"\"\"\n",
    "    return F.pad(x, (pad_w, pad_w, pad_h, pad_h), mode='reflect')\n",
    "\n",
    "\n",
    "def _reflection_pad_3d(x: Tensor, pad_d: int, pad_w: int, pad_h: int) -> Tensor:\n",
    "    \"\"\"Pads a 5D tensor with reflection padding.\"\"\"\n",
    "    return F.pad(x, (pad_w, pad_w, pad_h, pad_h, pad_d, pad_d), mode='reflect')\n",
    "\n",
    "\n",
    "def _ssim_check_inputs(preds: Tensor, target: Tensor, is_3d: bool) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Update and returns variables required to compute Structural Similarity Index Measure.\"\"\"\n",
    "    if preds.dtype != target.dtype:\n",
    "        target = target.to(preds.dtype)\n",
    "    if preds.shape != target.shape:\n",
    "        raise ValueError(f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\")\n",
    "    expected_dim = 5 if is_3d else 4\n",
    "    if len(preds.shape) != expected_dim:\n",
    "        raise ValueError(f\"Expected `preds` and `target` to have {'BxCxDxHxW' if is_3d else 'BxCxHxW'} shape, but got {preds.shape}.\")\n",
    "    return preds, target\n",
    "\n",
    "\n",
    "class StructuralSimilarityIndexMeasure(Metric):\n",
    "    \"\"\"Compute Structural Similarity Index Measure (SSIM) for 2D or 3D images.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_3d: bool = False,\n",
    "        gaussian_kernel: bool = True,\n",
    "        sigma: Union[float, Sequence[float]] = 1.5,\n",
    "        kernel_size: Union[int, Sequence[int]] = 11,\n",
    "        reduction: Literal[\"elementwise_mean\", \"sum\", \"none\", None] = \"elementwise_mean\",\n",
    "        data_range: Optional[Union[float, Tuple[float, float]]] = None,\n",
    "        k1: float = 0.01,\n",
    "        k2: float = 0.03,\n",
    "        return_full_image: bool = False,\n",
    "        return_contrast_sensitivity: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.is_3d = is_3d\n",
    "        self.gaussian_kernel = gaussian_kernel\n",
    "        self.sigma = sigma\n",
    "        self.kernel_size = kernel_size\n",
    "        self.reduction = reduction\n",
    "        self.data_range = data_range\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.return_full_image = return_full_image\n",
    "        self.return_contrast_sensitivity = return_contrast_sensitivity\n",
    "\n",
    "        valid_reduction = (\"elementwise_mean\", \"sum\", \"none\", None)\n",
    "        if reduction not in valid_reduction:\n",
    "            raise ValueError(f\"Argument `reduction` must be one of {valid_reduction}, but got {reduction}\")\n",
    "\n",
    "        if reduction in (\"elementwise_mean\", \"sum\"):\n",
    "            self.add_state(\"similarity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        else:\n",
    "            self.add_state(\"similarity\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "        if return_contrast_sensitivity or return_full_image:\n",
    "            self.add_state(\"image_return\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor) -> None:\n",
    "        \"\"\"Update state with predictions and targets.\"\"\"\n",
    "        preds, target = _ssim_check_inputs(preds, target, self.is_3d)\n",
    "        similarity_pack = self._ssim_update(preds, target)\n",
    "\n",
    "        if isinstance(similarity_pack, tuple):\n",
    "            similarity, image = similarity_pack\n",
    "        else:\n",
    "            similarity = similarity_pack\n",
    "\n",
    "        if self.return_contrast_sensitivity or self.return_full_image:\n",
    "            self.image_return.append(image)\n",
    "\n",
    "        if self.reduction in (\"elementwise_mean\", \"sum\"):\n",
    "            self.similarity += similarity.sum()\n",
    "            self.total += preds.shape[0]\n",
    "        else:\n",
    "            self.similarity.append(similarity)\n",
    "\n",
    "    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"Compute SSIM over state.\"\"\"\n",
    "        if self.reduction == \"elementwise_mean\":\n",
    "            similarity = self.similarity / self.total\n",
    "        elif self.reduction == \"sum\":\n",
    "            similarity = self.similarity\n",
    "        else:\n",
    "            similarity = torch.cat(self.similarity, dim=0)\n",
    "\n",
    "        if self.return_contrast_sensitivity or self.return_full_image:\n",
    "            image_return = torch.cat(self.image_return, dim=0)\n",
    "            return similarity, image_return\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def plot(\n",
    "        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[Any] = None\n",
    "    ) -> Any:\n",
    "        \"\"\"Plot a single or multiple values from the metric.\"\"\"\n",
    "        return self._plot(val, ax)\n",
    "\n",
    "    def _ssim_update(\n",
    "        self,\n",
    "        preds: Tensor,\n",
    "        target: Tensor\n",
    "    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"Compute Structural Similarity Index Measure.\"\"\"\n",
    "        if not isinstance(self.kernel_size, Sequence):\n",
    "            self.kernel_size = 3 * [self.kernel_size] if self.is_3d else 2 * [self.kernel_size]\n",
    "        if not isinstance(self.sigma, Sequence):\n",
    "            self.sigma = 3 * [self.sigma] if self.is_3d else 2 * [self.sigma]\n",
    "\n",
    "        if len(self.kernel_size) != (3 if self.is_3d else 2):\n",
    "            raise ValueError(f\"`kernel_size` should have {3 if self.is_3d else 2} dimensions, but got {len(self.kernel_size)}\")\n",
    "        if len(self.sigma) != (3 if self.is_3d else 2):\n",
    "            raise ValueError(f\"`sigma` should have {3 if self.is_3d else 2} dimensions, but got {len(self.sigma)}\")\n",
    "\n",
    "        if self.return_full_image and self.return_contrast_sensitivity:\n",
    "            raise ValueError(\"Arguments `return_full_image` and `return_contrast_sensitivity` are mutually exclusive.\")\n",
    "\n",
    "        if any(x % 2 == 0 or x <= 0 for x in self.kernel_size):\n",
    "            raise ValueError(f\"Expected `kernel_size` to have odd positive numbers. Got {self.kernel_size}\")\n",
    "\n",
    "        if any(y <= 0 for y in self.sigma):\n",
    "            raise ValueError(f\"Expected `sigma` to have positive numbers. Got {self.sigma}\")\n",
    "\n",
    "        if self.data_range is None:\n",
    "            self.data_range = max(preds.max() - preds.min(), target.max() - target.min())\n",
    "        elif isinstance(self.data_range, tuple):\n",
    "            preds = torch.clamp(preds, min=self.data_range[0], max=self.data_range[1])\n",
    "            target = torch.clamp(target, min=self.data_range[0], max=self.data_range[1])\n",
    "            self.data_range = self.data_range[1] - self.data_range[0]\n",
    "\n",
    "        c1 = (self.k1 * self.data_range) ** 2\n",
    "        c2 = (self.k2 * self.data_range) ** 2\n",
    "        device = preds.device\n",
    "\n",
    "        channel = preds.size(1)\n",
    "        dtype = preds.dtype\n",
    "        gauss_kernel_size = [int(3.5 * s + 0.5) * 2 + 1 for s in self.sigma]\n",
    "\n",
    "        if self.is_3d:\n",
    "            pad_h = (gauss_kernel_size[0] - 1) // 2\n",
    "            pad_w = (gauss_kernel_size[1] - 1) // 2\n",
    "            pad_d = (gauss_kernel_size[2] - 1) // 2\n",
    "\n",
    "            preds = _reflection_pad_3d(preds, pad_d, pad_w, pad_h)\n",
    "            target = _reflection_pad_3d(target, pad_d, pad_w, pad_h)\n",
    "\n",
    "            if self.gaussian_kernel:\n",
    "                kernel = _gaussian_kernel_3d(channel, gauss_kernel_size, self.sigma, dtype, device)\n",
    "            else:\n",
    "                kernel = torch.ones((channel, 1, *self.kernel_size), dtype=dtype, device=device) / torch.prod(\n",
    "                    torch.tensor(self.kernel_size, dtype=dtype, device=device)\n",
    "                )\n",
    "        else:\n",
    "            pad_h = (gauss_kernel_size[0] - 1) // 2\n",
    "            pad_w = (gauss_kernel_size[1] - 1) // 2\n",
    "\n",
    "            preds = _reflection_pad_2d(preds, pad_h, pad_w)\n",
    "            target = _reflection_pad_2d(target, pad_h, pad_w)\n",
    "\n",
    "            if self.gaussian_kernel:\n",
    "                kernel = _gaussian_kernel_2d(channel, self.kernel_size[0], self.sigma[0], dtype, device)\n",
    "            else:\n",
    "                kernel = torch.ones((channel, 1, *self.kernel_size), dtype=dtype, device=device) / torch.prod(\n",
    "                    torch.tensor(self.kernel_size, dtype=dtype, device=device)\n",
    "                )\n",
    "\n",
    "        input_list = torch.cat((preds, target, preds * preds, target * target, preds * target))\n",
    "\n",
    "        outputs = F.conv3d(input_list, kernel, groups=channel) if self.is_3d else F.conv2d(input_list, kernel, groups=channel)\n",
    "\n",
    "        output_list = outputs.split(preds.shape[0])\n",
    "\n",
    "        mu_pred = output_list[0]\n",
    "        mu_target = output_list[1]\n",
    "        mu_pred_sq = mu_pred.pow(2)\n",
    "        mu_target_sq = mu_target.pow(2)\n",
    "        mu_pred_target = mu_pred * mu_target\n",
    "\n",
    "        sigma_pred_sq = torch.clamp(output_list[2] - mu_pred_sq, min=0.0)\n",
    "        sigma_target_sq = torch.clamp(output_list[3] - mu_target_sq, min=0.0)\n",
    "        sigma_pred_target = torch.clamp(output_list[4] - mu_pred_target, min=0.0)\n",
    "\n",
    "        upper = 2 * sigma_pred_target.to(dtype) + c2\n",
    "        lower = (sigma_pred_sq + sigma_target_sq).to(dtype) + c2\n",
    "\n",
    "        ssim_idx_full_image = ((2 * mu_pred_target + c1) * upper) / ((mu_pred_sq + mu_target_sq + c1) * lower)\n",
    "\n",
    "        if self.is_3d:\n",
    "            ssim_idx = ssim_idx_full_image[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]\n",
    "        else:\n",
    "            ssim_idx = ssim_idx_full_image[..., pad_h:-pad_h, pad_w:-pad_w]\n",
    "\n",
    "        if self.return_contrast_sensitivity:\n",
    "            contrast_sensitivity = upper / lower\n",
    "            if self.is_3d:\n",
    "                contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]\n",
    "            else:\n",
    "                contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w]\n",
    "            return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), contrast_sensitivity.reshape(\n",
    "                contrast_sensitivity.shape[0], -1\n",
    "            ).mean(-1)\n",
    "\n",
    "        if self.return_full_image:\n",
    "            return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), ssim_idx_full_image\n",
    "\n",
    "        return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1)\n",
    "\n",
    "\n",
    "# Example Usage for 2D and 3D SSIM\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Example for 2D SSIM\n",
    "    ssim2d = StructuralSimilarityIndexMeasure(is_3d=False, data_range=1.0).to(device)\n",
    "    preds_2d = torch.rand([3, 1, 256, 256], device=device)\n",
    "    target_2d = preds_2d * 0.75\n",
    "    ssim2d.update(preds_2d, target_2d)\n",
    "    ssim_value_2d = ssim2d.compute()\n",
    "    print(f\"SSIM2D: {ssim_value_2d.item()}\")\n",
    "\n",
    "    # Example for 3D SSIM\n",
    "    ssim3d = StructuralSimilarityIndexMeasure(is_3d=True, data_range=1.0).to(device)\n",
    "    preds_3d = torch.rand([16, 1, 32, 256, 256], device=device)\n",
    "    target_3d = preds_3d * 0.75\n",
    "    ssim3d.update(preds_3d, target_3d)\n",
    "    ssim_value_3d = ssim3d.compute()\n",
    "    print(f\"SSIM3D: {ssim_value_3d.item()}\")\n",
    "\n",
    "    loss = torch.nn.L1Loss(reduction='mean').to(device)\n",
    "    output = loss(preds_2d, target_2d)\n",
    "\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03696852922439575\n"
     ]
    }
   ],
   "source": [
    "from models.enhancement_loss import *\n",
    "preds_2d = torch.rand([3, 8, 1, 256, 256], device=device)\n",
    "target_2d = preds_2d * 0.75\n",
    "\n",
    "loss_func = Weighted_MSSSIM_Complex_Loss(device='cuda')\n",
    "print(loss_func(preds_2d, target_2d, None).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 64, 224, 224])\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         7.84%     630.000us        29.29%       2.353ms       2.353ms       0.000us         0.00%       6.137ms       6.137ms             1  \n",
      "                                              aten::add         0.63%      51.000us         0.88%      71.000us      23.667us       5.123ms        83.48%       5.123ms       1.708ms             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.763ms        77.61%       4.763ms       2.381ms             2  \n",
      "                                      aten::convolution         0.65%      52.000us        18.08%       1.452ms     242.000us       0.000us         0.00%       1.014ms     169.000us             6  \n",
      "                                     aten::_convolution         6.44%     517.000us        17.43%       1.400ms     233.333us       0.000us         0.00%       1.014ms     169.000us             6  \n",
      "                                           aten::conv2d         1.66%     133.000us        18.27%       1.468ms     244.667us       0.000us         0.00%     779.000us     129.833us             6  \n",
      "                                aten::cudnn_convolution         1.69%     136.000us         2.20%     177.000us      59.000us     708.000us        11.54%     708.000us     236.000us             3  \n",
      "         cudnn_ampere_scudnn_128x32_relu_interior_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     696.000us        11.34%     696.000us     232.000us             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     360.000us         5.87%     360.000us     360.000us             1  \n",
      "                                aten::_conv_depthwise2d         0.59%      47.000us         8.79%     706.000us     235.333us     306.000us         4.99%     306.000us     102.000us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.033ms\n",
      "Self CUDA time total: 6.137ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-14 21:54:11 1201256:1201256 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-07-14 21:54:11 1201256:1201256 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-07-14 21:54:11 1201256:1201256 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "export_stacks() requires with_stack=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(prof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# To save the profiler output to a file\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofiler_output.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mself_cuda_time_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/profiler/profiler.py:173\u001b[0m, in \u001b[0;36m_KinetoProfile.export_stacks\u001b[0;34m(self, path, metric)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save stack traces in a file in a format suitable for visualization.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    - ./flamegraph.pl --title \"CPU time\" --countname \"us.\" profiler.stacks > perf_viz.svg\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/autograd/profiler.py:289\u001b[0m, in \u001b[0;36mprofile.export_stacks\u001b[0;34m(self, path, metric)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_finish()\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected profiling results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_stack, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_stacks() requires with_stack=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events\u001b[38;5;241m.\u001b[39mexport_stacks(path, metric)\n",
      "\u001b[0;31mAssertionError\u001b[0m: export_stacks() requires with_stack=True"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "class MSDC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, stride, activation='relu6'):\n",
    "        super(MSDC, self).__init__()\n",
    "\n",
    "        self.dwconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size, stride, kernel_size // 2, groups=in_channels, bias=False),\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                #nn.BatchNorm2d(out_channels),\n",
    "                #nn.ReLU(inplace=True) if activation == 'relu' else nn.ReLU6(inplace=True)\n",
    "            )\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolution layers in a loop and sum their outputs\n",
    "        outputs = sum(dwconv(x) for dwconv in self.dwconvs)\n",
    "        return outputs\n",
    "\n",
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "@torch.jit.script\n",
    "def channel_shuffle(x, groups: int):\n",
    "    batchsize, num_channels, height, width = x.size()\n",
    "    channels_per_group = num_channels // groups\n",
    "    x = x.view(batchsize, groups, channels_per_group, height, width).transpose(1, 2).contiguous()\n",
    "    return x.view(batchsize, -1, height, width)\n",
    "\n",
    "class MSCB(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale convolution block (MSCB)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride, kernel_sizes=[3], expansion_factor=1, activation='relu6'):\n",
    "        super(MSCB, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.activation = activation\n",
    "\n",
    "        self.ex_channels = int(self.in_channels * self.expansion_factor)\n",
    "        \n",
    "        # Expand and apply multi-scale depthwise convolutions\n",
    "        self.msdc = MSDC(self.ex_channels, self.out_channels, self.kernel_sizes, self.stride, self.activation)\n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.msdc(x)\n",
    "        out = channel_shuffle(out, gcd(self.ex_channels,self.out_channels))\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    model = MSCB(in_channels=3, out_channels=64, stride=1, kernel_sizes=[1, 3, 5], expansion_factor=1).cuda()\n",
    "    x = torch.randn(24, 3, 224, 224).cuda()  # Example input tensor\n",
    "\n",
    "    # Using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            output = model(x)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "    # To save the profiler output to a file\n",
    "    prof.export_stacks(\"profiler_output.txt\", metric=\"self_cuda_time_total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 64, 224, 224])\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         3.24%     851.000us         9.69%       2.549ms       2.549ms       0.000us         0.00%      22.931ms      22.931ms             1  \n",
      "                                        aten::hardtanh_         0.14%      38.000us         0.73%     191.000us      63.667us       0.000us         0.00%       6.905ms       2.302ms             3  \n",
      "                                       aten::batch_norm         0.03%       7.000us         0.82%     216.000us      72.000us       0.000us         0.00%       5.931ms       1.977ms             3  \n",
      "                           aten::_batch_norm_impl_index         0.05%      13.000us         0.79%     209.000us      69.667us       0.000us         0.00%       5.931ms       1.977ms             3  \n",
      "                                 aten::cudnn_batch_norm         0.42%     111.000us         0.75%     196.000us      65.333us       5.931ms        25.86%       5.931ms       1.977ms             3  \n",
      "void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, ...         0.00%       0.000us         0.00%       0.000us       0.000us       5.931ms        25.86%       5.931ms       1.977ms             3  \n",
      "                                         aten::hardtanh         0.03%       9.000us         0.23%      60.000us      20.000us       0.000us         0.00%       5.745ms       1.915ms             3  \n",
      "                                            aten::clamp         0.12%      32.000us         0.19%      51.000us      17.000us       5.745ms        25.05%       5.745ms       1.915ms             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       5.745ms        25.05%       5.745ms       1.915ms             3  \n",
      "                                             aten::add_         0.24%      64.000us         0.36%      96.000us      19.200us       5.154ms        22.48%       5.154ms       1.031ms             5  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 26.302ms\n",
      "Self CUDA time total: 22.931ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-14 21:51:27 1201256:1201256 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-07-14 21:51:27 1201256:1201256 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-07-14 21:51:27 1201256:1201256 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "export_stacks() requires with_stack=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(prof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# To save the profiler output to a file\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofiler_output.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mself_cuda_time_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/profiler/profiler.py:173\u001b[0m, in \u001b[0;36m_KinetoProfile.export_stacks\u001b[0;34m(self, path, metric)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save stack traces in a file in a format suitable for visualization.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    - ./flamegraph.pl --title \"CPU time\" --countname \"us.\" profiler.stacks > perf_viz.svg\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_stacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/autograd/profiler.py:289\u001b[0m, in \u001b[0;36mprofile.export_stacks\u001b[0;34m(self, path, metric)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_finish()\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected profiling results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_stack, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_stacks() requires with_stack=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events\u001b[38;5;241m.\u001b[39mexport_stacks(path, metric)\n",
      "\u001b[0;31mAssertionError\u001b[0m: export_stacks() requires with_stack=True"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "class MSDC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, stride, activation='relu6', dw_parallel=True):\n",
    "        super(MSDC, self).__init__()\n",
    "\n",
    "        self.dwconvs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size // 2, groups=gcd(in_channels, out_channels), bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU6(inplace=True) if activation == 'relu6' else nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.dwconvs[0](x)\n",
    "        for dwconv in self.dwconvs[1:]:\n",
    "            outputs.add_(dwconv(x))  # In-place addition to reduce memory usage\n",
    "        return outputs\n",
    "\n",
    "@torch.jit.script\n",
    "def channel_shuffle(x, groups: int):\n",
    "    batchsize, num_channels, height, width = x.size()\n",
    "    channels_per_group = num_channels // groups\n",
    "    x = x.view(batchsize, groups, channels_per_group, height, width).transpose(1, 2).contiguous()\n",
    "    return x.view(batchsize, -1, height, width)\n",
    "\n",
    "class MSCB(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale convolution block (MSCB)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride, kernel_sizes=[3], expansion_factor=1, dw_parallel=True, add=True, activation='relu6'):\n",
    "        super(MSCB, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.dw_parallel = dw_parallel\n",
    "        self.add = add\n",
    "        self.activation = activation\n",
    "        self.n_scales = len(self.kernel_sizes)\n",
    "\n",
    "        self.ex_channels = int(self.in_channels * self.expansion_factor)\n",
    "        self.msdc = MSDC(self.ex_channels, self.out_channels, self.kernel_sizes, self.stride, self.activation, dw_parallel=self.dw_parallel)\n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.msdc(x)\n",
    "        out = channel_shuffle(out, gcd(self.ex_channels,self.out_channels))\n",
    "        return out\n",
    "\n",
    "# Helper function to compute the greatest common divisor (gcd)\n",
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "# Example usage with profiler\n",
    "if __name__ == \"__main__\":\n",
    "    model = MSCB(in_channels=3, out_channels=64, stride=1, kernel_sizes=[1, 3, 5], expansion_factor=1).cuda()\n",
    "    x = torch.randn(24, 3, 224, 224).cuda()  # Example input tensor\n",
    "\n",
    "    # Using PyTorch profiler\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            output = model(x)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "    # To save the profiler output to a file\n",
    "    prof.export_stacks(\"profiler_output.txt\", metric=\"self_cuda_time_total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-07-16 15:23:11 1717881:1717881 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-07-16 15:23:12 1717881:1717881 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-07-16 15:23:12 1717881:1717881 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profile(activities\u001b[38;5;241m=\u001b[39m[ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA], record_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(prof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda_time_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 124\u001b[0m, in \u001b[0;36mMSUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39mskip_connection\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    123\u001b[0m     concat_skip \u001b[38;5;241m=\u001b[39m skip_connection \u001b[38;5;241m+\u001b[39m x \u001b[38;5;66;03m#torch.cat((skip_connection, x), dim=1)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m(concat_skip)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/cnnt_venv/lib/python3.11/site-packages/torch/nn/modules/container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def channel_shuffle(x, groups: int):\n",
    "    batchsize, num_channels, depth, height, width = x.size()\n",
    "    channels_per_group = num_channels // groups\n",
    "    x = x.view(batchsize, groups, channels_per_group, depth, height, width).transpose(1, 2).contiguous()\n",
    "    return x.view(batchsize, -1, depth, height, width)\n",
    "\n",
    "class MSDC3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, stride, activation='relu6', dw_parallel=True):\n",
    "        super(MSDC3D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.dwconvs = nn.ModuleList([\n",
    "            nn.Conv3d(self.in_channels, self.in_channels, kernel_size, stride, kernel_size // 2, groups=in_channels, bias=False)\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.pointwise_conv = nn.Conv3d(in_channels, out_channels, 1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm3d(out_channels)\n",
    "        self.activation = nn.ReLU(inplace=True) if activation == 'relu' else nn.ReLU6(inplace=True)\n",
    "        \n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        depthwise_outs = [dwconv(x) for dwconv in self.dwconvs]\n",
    "        summed = sum(depthwise_outs)\n",
    "        pointwise_out = self.pointwise_conv(summed)\n",
    "        output = self.batch_norm(pointwise_out)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "class MSCB3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale convolution block (MSCB) \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride, kernel_sizes=[3], expansion_factor=1, dw_parallel=True, add=True, activation='relu6'):\n",
    "        super(MSCB3D, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.dw_parallel = dw_parallel\n",
    "        self.add = add\n",
    "        self.activation = activation\n",
    "        self.n_scales = len(self.kernel_sizes)\n",
    "\n",
    "        self.ex_channels = int(self.in_channels * self.expansion_factor)\n",
    "        self.msdc = MSDC3D(self.ex_channels, self.out_channels, self.kernel_sizes, self.stride, self.activation, dw_parallel=self.dw_parallel)\n",
    "        self.init_weights('normal')\n",
    "    \n",
    "    def init_weights(self, scheme=''):\n",
    "        # Initialize weights if necessary\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.msdc(x)\n",
    "        return out\n",
    "\n",
    "class MSCBLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n=1, stride=1, kernel_sizes=[3], expansion_factor=1, dw_parallel=True, add=True, activation='relu6'):\n",
    "        super(MSCBLayer3D, self).__init__()\n",
    "        layers = [MSCB3D(in_channels, out_channels, stride, kernel_sizes, expansion_factor, dw_parallel, add, activation)]\n",
    "        for _ in range(1, n):\n",
    "            layers.append(MSCB3D(out_channels, out_channels, 1, kernel_sizes, expansion_factor, dw_parallel, add, activation))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MSUNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[32, 64, 128, 256], kernel_sizes=[1, 3, 5]):\n",
    "        super(MSUNet3D, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for feature in features:\n",
    "            self.encoder.append(MSCBLayer3D(in_channels, feature, n=1, kernel_sizes=kernel_sizes))\n",
    "            in_channels = feature\n",
    "\n",
    "        for c_idx in range (len(features)-1):\n",
    "            self.decoder.append(\n",
    "                nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            )\n",
    "            self.decoder.append(MSCBLayer3D(features[-(c_idx+1)], features[-(c_idx+2)], n=1, kernel_sizes=kernel_sizes))\n",
    "\n",
    "        self.upsample3d = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        \n",
    "        self.bottleneck = MSCBLayer3D(features[-1], features[-1], n=1, kernel_sizes=kernel_sizes)\n",
    "        #self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])\n",
    "            concat_skip = skip_connection + x #torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.decoder[idx+1](concat_skip)\n",
    "\n",
    "        return self.upsample3d(x) #self.final_conv(x)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = MSUNet3D(in_channels=1, out_channels=1)\n",
    "    x = torch.randn(1, 1, 16, 128, 128)  # Example input tensor\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            output = model(x)\n",
    "    print(output.shape)\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "    prof.export_stacks(\"profiler_output.txt\", metric=\"self_cuda_time_total\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
